<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>hipe4ml.model_handler API documentation</title>
<meta name="description" content="Module containing the class used for wrapping the models from different
ML libraries to build a new model with common methods" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hipe4ml.model_handler</code></h1>
</header>
<section id="section-intro">
<p>Module containing the class used for wrapping the models from different
ML libraries to build a new model with common methods</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Module containing the class used for wrapping the models from different
ML libraries to build a new model with common methods
&#34;&#34;&#34;
import inspect
import pickle

import numpy as np
from bayes_opt import BayesianOptimization
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score

import hipe4ml.tree_handler


class ModelHandler:
    &#34;&#34;&#34;
    Class used for wrapping the models from different ML libraries to
    build a new model with common methods. Currently only XGBoost
    (through it&#39;s sklearn interface) and sklearn models are supported.

    Parameters
    -------------------------------------------------
    input_model: xgboost or sklearn model

    training_columns: list
        Contains the name of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    model_params: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...
    &#34;&#34;&#34;

    def __init__(self, input_model=None, training_columns=None, model_params=None):
        self.model = input_model
        self.training_columns = training_columns
        self.model_params = model_params
        self._n_classes = None

        if self.model is not None:
            self.model_string = inspect.getmodule(self.model).__name__.partition(&#39;.&#39;)[0]

            if self.model_params is None:
                self.model_params = self.model.get_params()
            else:
                self.model.set_params(**self.model_params)

    def set_model_params(self, model_params):
        &#34;&#34;&#34;
        Set the model (hyper-)parameters

        Parameters
        ------------------------------------
        model_params: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        self.model_params = model_params
        self.model.set_params(**self.model_params)

    def get_model_params(self):
        &#34;&#34;&#34;
        Get the model (hyper-)parameters

        Returns
        ------------------------------------
        out: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        return self.model.get_params()

    def set_training_columns(self, training_columns):
        &#34;&#34;&#34;
        Set the features used for the training process

        Parameters
        ------------------------------------
        training_columns: list
            Contains the name of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;
        self.training_columns = training_columns

    def get_training_columns(self):
        &#34;&#34;&#34;
        Get the features used for the training process

        Returns
        ------------------------------------
        out: list
            Names of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;

        return self.training_columns

    def get_original_model(self):
        &#34;&#34;&#34;
        Get the original unwrapped model

        Returns
        ---------------------------
        out: sklearn or XGBoost model
        &#34;&#34;&#34;
        return self.model

    def get_model_module(self):
        &#34;&#34;&#34;
        Get the string containing the name
        of the model module

        Returns
        ---------------------------
        out: str
            Name of the model module
        &#34;&#34;&#34;
        return self.model_string

    def get_n_classes(self):
        &#34;&#34;&#34;
        Get the number of classes

        Returns
        ---------------------------
        out: int
            Number of classes
        &#34;&#34;&#34;
        return self._n_classes

    def fit(self, x_train, y_train):
        &#34;&#34;&#34;
        Fit Model

        Parameters
        ---------------------------
        x_train: array-like, sparse matrix
            Training data

        y_train: array-like, sparse matrix
            Target data
        &#34;&#34;&#34;
        n_classes = len(np.unique(y_train))
        self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(x_train.columns)

        self.model.fit(x_train[self.training_columns], y_train)

    def predict(self, x_test, output_margin=True):
        &#34;&#34;&#34;
        Return model prediction for the array x_test

        Parameters
        --------------------------------------
        x_test: hipe4ml tree_handler, array-like, sparse matrix
            The input sample.

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned

        Returns
        ---------------------------------------
        out: numpy array
            Model predictions
        &#34;&#34;&#34;
        if isinstance(x_test, hipe4ml.tree_handler.TreeHandler):
            x_test = x_test.get_data_frame()

        x_test = x_test[self.training_columns]

        if output_margin:
            if self.model_string == &#39;xgboost&#39;:
                pred = self.model.predict(x_test, True)
            if self.model_string == &#39;sklearn&#39;:
                pred = self.model.decision_function(x_test).ravel()
        else:
            pred = self.model.predict_proba(x_test)
            # in case of binary classification return only the scores of
            # the signal class
            if pred.shape[1] &lt;= 2:
                pred = pred[:, 1]

        return pred

    def train_test_model(self, data, return_prediction=False, output_margin=False, average=&#39;macro&#39;,
                         multi_class_opt=&#39;raise&#39;):
        &#34;&#34;&#34;
        Perform the training and the testing of the model. The model performance is estimated
        using the ROC AUC metric

        Parameters
        ----------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        return_prediction: bool
            If True Model predictions on the test set are
            returned

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned

        average: string
            Option for the average of ROC AUC scores used only in case of multi-classification.
            You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

        multi_class_opt: string
            Option to compute ROC AUC scores used only in case of multi-classification.
            The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

        Returns
        ---------------------------------------
        out: numpy array or None
            If return_prediction==True, Model predictions on the test set are
            returned

        &#34;&#34;&#34;

        # get number of classes
        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        print(&#39;Number of detected classes:&#39;, n_classes)

        # final training with the optimized hyperparams
        print(&#39;Training the final model: ...&#39;, end=&#39;\r&#39;)
        self.fit(data[0], data[1])
        print(&#39;Training the final model: Done!&#39;)
        print(&#39;Testing the model: ...&#39;, end=&#39;\r&#39;)
        y_pred = self.predict(data[2], output_margin=output_margin)
        roc_score = roc_auc_score(
            data[3], y_pred, average=average, multi_class=multi_class_opt)
        print(&#39;Testing the model: Done!&#39;)

        print(f&#39;ROC_AUC_score: {roc_score:.6f}&#39;)
        print(&#39;==============================&#39;)
        if return_prediction:
            return y_pred
        return None

    def evaluate_hyperparams(self, data, opt_params, scoring, nfold=5, njobs=None):
        &#34;&#34;&#34;
        Calculate the cross-validation score for a set of hyper-parameters

        Parameters
        ------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        opt_params: dict
            Hyperparameters to be optimized. For
            example: max_depth, learning_rate,
            n_estimators, gamma, min_child_weight,
            subsample, colsample_bytree

        scoring: string, callable or None
            A string (see sklearn model evaluation documentation:
            https://scikit-learn.org/stable/modules/model_evaluation.html)
            or a scorer callable object / function with signature scorer(estimator, X, y)
            which should return only a single value

        nfold: int
            Number of folds to perform the cross validation

        njobs: int or None
            Number of CPUs to use to perform computation.
            Set to -1 to use all CPUs

        Returns
        ---------------------------------------------------------
        out: float
            Cross validation score evaluated using
            the ROC AUC metrics
        &#34;&#34;&#34;
        opt_params = self.__cast_model_params(opt_params)
        params = {**self.model_params, **opt_params}
        self.model.set_params(**params)
        if self.training_columns is not None:
            return np.mean(cross_val_score(self.model, data[0][self.training_columns], data[1],
                                           cv=nfold, scoring=scoring, n_jobs=njobs))
        return np.mean(cross_val_score(self.model, data[0], data[1], cv=nfold, scoring=scoring, n_jobs=njobs))

    def optimize_params_bayes(self, data, hyperparams_ranges, scoring, nfold=5, init_points=5,
                              n_iter=5, njobs=None):
        &#34;&#34;&#34;
        Perform Bayesian optimization and update the model hyper-parameters
        with the best ones

        Parameters
        ------------------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        hyperparams_ranges: dict
            Hyperparameter ranges(in tuples).
            Important: the type of the params must be preserved
            when passing the ranges.
            For example:
            dict={
                &#39;max_depth&#39;:(10,100)
                &#39;learning_rate&#39;: (0.01,0.03)
            }

        scoring: string, callable or None
            A string (see sklearn model evaluation documentation:
            https://scikit-learn.org/stable/modules/model_evaluation.html)
            or a scorer callable object / function with signature scorer(estimator, X, y)
            which should return only a single value.
            In binary classification &#39;roc_auc&#39; is suggested.
            In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
            ‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
            For more information see
            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

        nfold: int
            Number of folds to calculate the cross validation error

        init_points: int
            How many steps of random exploration you want to perform.
            Random exploration can help by diversifying the exploration space

        n_iter: int
            How many steps for bayesian optimization of the target function.
            Bigger n_iter results in better description of thetarget function

        njobs: int or None
            Number of CPUs to perform computation used in the score evaluation
            with cross-validation. Set to -1 to use all CPUs
        &#34;&#34;&#34;
        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(data[0].columns)

        start_params = {}
        for key in hyperparams_ranges:
            start_params[key] = hyperparams_ranges[key][0]
        self.set_model_params({**self.model_params, **start_params})

        # just an helper function
        def hyperparams_crossvalidation(**kwargs):
            return self.evaluate_hyperparams(data, kwargs, scoring, nfold, njobs)
        print(&#39;&#39;)

        optimizer = BayesianOptimization(f=hyperparams_crossvalidation, pbounds=hyperparams_ranges,
                                         verbose=2, random_state=42)
        optimizer.maximize(init_points=init_points, n_iter=n_iter, acq=&#39;poi&#39;)
        print(&#39;&#39;)

        # extract and show the results of the optimization
        max_params = {key: None for key in hyperparams_ranges.keys()}
        for key in max_params.keys():
            max_params[key] = optimizer.max[&#39;params&#39;][key]
        print(f&#34;Best target: {optimizer.max[&#39;target&#39;]:.6f}&#34;)
        print(f&#39;Best parameters: {max_params}&#39;)
        self.set_model_params({**self.model_params, **self.__cast_model_params(max_params)})

    def __cast_model_params(self, params):
        &#34;&#34;&#34;
        Check if each model parameter is defined as integer
        and change the parameter dictionary consequently
        Be careful: some libraries like XGBoost do not have
        default parameters with the correct type, it is up to
        the user of this function to make sure that the types
        are correctly initiated in the model before casting
        the params dictionary.

        Parameters
        -----------------------------------------------------
        params: dict
            Hyperparameter values. For
            example: max_depth, learning_rate,
            n_estimators, gamma, min_child_weight,
            subsample, colsample_bytree

        Returns
        -----------------------------------------------------
        out: dict
            Hyperparameter values updated
        &#34;&#34;&#34;
        for key in params.keys():
            if key in self.model.get_params():
                def_val = self.model.get_params()[key]
                params[key] = type(def_val)(round(params[key]) if isinstance(def_val, int) else params[key])
        return params

    def dump_original_model(self, filename, xgb_format=False):
        &#34;&#34;&#34;
        Save the trained model into a pickle
        file. Only for xgboost models it is also given
        the possibility to save them into a .model file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved

        xgb_format : bool
            If True saves the xgboost model into a .model file
        &#34;&#34;&#34;
        if xgb_format is False:
            with open(filename, &#34;wb&#34;) as output_file:
                pickle.dump(self.model, output_file)
        else:
            if self.model_string == &#39;xgboost&#39;:
                self.model.save_model(filename)
            else:
                print(&#34;File not saved: only xgboost models support the .model extension&#34;)

    def dump_model_handler(self, filename):
        &#34;&#34;&#34;
        Save the model handler into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;wb&#34;) as output_file:
            pickle.dump(self, output_file)

    def load_model_handler(self, filename):
        &#34;&#34;&#34;
        Load a model handler saved into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;rb&#34;) as input_file:
            loaded_model = pickle.load(input_file)
            self.model = loaded_model.get_original_model()
            self.training_columns = loaded_model.get_training_columns()
            self.model_params = loaded_model.get_model_params()
            self.model.set_params(**self.model_params)
            self.model_string = loaded_model.get_model_module()
            self._n_classes = loaded_model.get_n_classes()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hipe4ml.model_handler.ModelHandler"><code class="flex name class">
<span>class <span class="ident">ModelHandler</span></span>
<span>(</span><span>input_model=None, training_columns=None, model_params=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class used for wrapping the models from different ML libraries to
build a new model with common methods. Currently only XGBoost
(through it's sklearn interface) and sklearn models are supported.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_model</code></strong> :&ensp;<code>xgboost</code> or <code>sklearn model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>training_columns</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the name of the features used for the training.
Example: ['dEdx', 'pT', 'ct']</dd>
<dt><strong><code>model_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model hyper-parameter values. For
example (XGBoost): max_depth, learning_rate,
n_estimators, gamma, min_child_weight, &hellip;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelHandler:
    &#34;&#34;&#34;
    Class used for wrapping the models from different ML libraries to
    build a new model with common methods. Currently only XGBoost
    (through it&#39;s sklearn interface) and sklearn models are supported.

    Parameters
    -------------------------------------------------
    input_model: xgboost or sklearn model

    training_columns: list
        Contains the name of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    model_params: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...
    &#34;&#34;&#34;

    def __init__(self, input_model=None, training_columns=None, model_params=None):
        self.model = input_model
        self.training_columns = training_columns
        self.model_params = model_params
        self._n_classes = None

        if self.model is not None:
            self.model_string = inspect.getmodule(self.model).__name__.partition(&#39;.&#39;)[0]

            if self.model_params is None:
                self.model_params = self.model.get_params()
            else:
                self.model.set_params(**self.model_params)

    def set_model_params(self, model_params):
        &#34;&#34;&#34;
        Set the model (hyper-)parameters

        Parameters
        ------------------------------------
        model_params: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        self.model_params = model_params
        self.model.set_params(**self.model_params)

    def get_model_params(self):
        &#34;&#34;&#34;
        Get the model (hyper-)parameters

        Returns
        ------------------------------------
        out: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        return self.model.get_params()

    def set_training_columns(self, training_columns):
        &#34;&#34;&#34;
        Set the features used for the training process

        Parameters
        ------------------------------------
        training_columns: list
            Contains the name of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;
        self.training_columns = training_columns

    def get_training_columns(self):
        &#34;&#34;&#34;
        Get the features used for the training process

        Returns
        ------------------------------------
        out: list
            Names of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;

        return self.training_columns

    def get_original_model(self):
        &#34;&#34;&#34;
        Get the original unwrapped model

        Returns
        ---------------------------
        out: sklearn or XGBoost model
        &#34;&#34;&#34;
        return self.model

    def get_model_module(self):
        &#34;&#34;&#34;
        Get the string containing the name
        of the model module

        Returns
        ---------------------------
        out: str
            Name of the model module
        &#34;&#34;&#34;
        return self.model_string

    def get_n_classes(self):
        &#34;&#34;&#34;
        Get the number of classes

        Returns
        ---------------------------
        out: int
            Number of classes
        &#34;&#34;&#34;
        return self._n_classes

    def fit(self, x_train, y_train):
        &#34;&#34;&#34;
        Fit Model

        Parameters
        ---------------------------
        x_train: array-like, sparse matrix
            Training data

        y_train: array-like, sparse matrix
            Target data
        &#34;&#34;&#34;
        n_classes = len(np.unique(y_train))
        self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(x_train.columns)

        self.model.fit(x_train[self.training_columns], y_train)

    def predict(self, x_test, output_margin=True):
        &#34;&#34;&#34;
        Return model prediction for the array x_test

        Parameters
        --------------------------------------
        x_test: hipe4ml tree_handler, array-like, sparse matrix
            The input sample.

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned

        Returns
        ---------------------------------------
        out: numpy array
            Model predictions
        &#34;&#34;&#34;
        if isinstance(x_test, hipe4ml.tree_handler.TreeHandler):
            x_test = x_test.get_data_frame()

        x_test = x_test[self.training_columns]

        if output_margin:
            if self.model_string == &#39;xgboost&#39;:
                pred = self.model.predict(x_test, True)
            if self.model_string == &#39;sklearn&#39;:
                pred = self.model.decision_function(x_test).ravel()
        else:
            pred = self.model.predict_proba(x_test)
            # in case of binary classification return only the scores of
            # the signal class
            if pred.shape[1] &lt;= 2:
                pred = pred[:, 1]

        return pred

    def train_test_model(self, data, return_prediction=False, output_margin=False, average=&#39;macro&#39;,
                         multi_class_opt=&#39;raise&#39;):
        &#34;&#34;&#34;
        Perform the training and the testing of the model. The model performance is estimated
        using the ROC AUC metric

        Parameters
        ----------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        return_prediction: bool
            If True Model predictions on the test set are
            returned

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned

        average: string
            Option for the average of ROC AUC scores used only in case of multi-classification.
            You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

        multi_class_opt: string
            Option to compute ROC AUC scores used only in case of multi-classification.
            The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

        Returns
        ---------------------------------------
        out: numpy array or None
            If return_prediction==True, Model predictions on the test set are
            returned

        &#34;&#34;&#34;

        # get number of classes
        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        print(&#39;Number of detected classes:&#39;, n_classes)

        # final training with the optimized hyperparams
        print(&#39;Training the final model: ...&#39;, end=&#39;\r&#39;)
        self.fit(data[0], data[1])
        print(&#39;Training the final model: Done!&#39;)
        print(&#39;Testing the model: ...&#39;, end=&#39;\r&#39;)
        y_pred = self.predict(data[2], output_margin=output_margin)
        roc_score = roc_auc_score(
            data[3], y_pred, average=average, multi_class=multi_class_opt)
        print(&#39;Testing the model: Done!&#39;)

        print(f&#39;ROC_AUC_score: {roc_score:.6f}&#39;)
        print(&#39;==============================&#39;)
        if return_prediction:
            return y_pred
        return None

    def evaluate_hyperparams(self, data, opt_params, scoring, nfold=5, njobs=None):
        &#34;&#34;&#34;
        Calculate the cross-validation score for a set of hyper-parameters

        Parameters
        ------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        opt_params: dict
            Hyperparameters to be optimized. For
            example: max_depth, learning_rate,
            n_estimators, gamma, min_child_weight,
            subsample, colsample_bytree

        scoring: string, callable or None
            A string (see sklearn model evaluation documentation:
            https://scikit-learn.org/stable/modules/model_evaluation.html)
            or a scorer callable object / function with signature scorer(estimator, X, y)
            which should return only a single value

        nfold: int
            Number of folds to perform the cross validation

        njobs: int or None
            Number of CPUs to use to perform computation.
            Set to -1 to use all CPUs

        Returns
        ---------------------------------------------------------
        out: float
            Cross validation score evaluated using
            the ROC AUC metrics
        &#34;&#34;&#34;
        opt_params = self.__cast_model_params(opt_params)
        params = {**self.model_params, **opt_params}
        self.model.set_params(**params)
        if self.training_columns is not None:
            return np.mean(cross_val_score(self.model, data[0][self.training_columns], data[1],
                                           cv=nfold, scoring=scoring, n_jobs=njobs))
        return np.mean(cross_val_score(self.model, data[0], data[1], cv=nfold, scoring=scoring, n_jobs=njobs))

    def optimize_params_bayes(self, data, hyperparams_ranges, scoring, nfold=5, init_points=5,
                              n_iter=5, njobs=None):
        &#34;&#34;&#34;
        Perform Bayesian optimization and update the model hyper-parameters
        with the best ones

        Parameters
        ------------------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        hyperparams_ranges: dict
            Hyperparameter ranges(in tuples).
            Important: the type of the params must be preserved
            when passing the ranges.
            For example:
            dict={
                &#39;max_depth&#39;:(10,100)
                &#39;learning_rate&#39;: (0.01,0.03)
            }

        scoring: string, callable or None
            A string (see sklearn model evaluation documentation:
            https://scikit-learn.org/stable/modules/model_evaluation.html)
            or a scorer callable object / function with signature scorer(estimator, X, y)
            which should return only a single value.
            In binary classification &#39;roc_auc&#39; is suggested.
            In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
            ‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
            For more information see
            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

        nfold: int
            Number of folds to calculate the cross validation error

        init_points: int
            How many steps of random exploration you want to perform.
            Random exploration can help by diversifying the exploration space

        n_iter: int
            How many steps for bayesian optimization of the target function.
            Bigger n_iter results in better description of thetarget function

        njobs: int or None
            Number of CPUs to perform computation used in the score evaluation
            with cross-validation. Set to -1 to use all CPUs
        &#34;&#34;&#34;
        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(data[0].columns)

        start_params = {}
        for key in hyperparams_ranges:
            start_params[key] = hyperparams_ranges[key][0]
        self.set_model_params({**self.model_params, **start_params})

        # just an helper function
        def hyperparams_crossvalidation(**kwargs):
            return self.evaluate_hyperparams(data, kwargs, scoring, nfold, njobs)
        print(&#39;&#39;)

        optimizer = BayesianOptimization(f=hyperparams_crossvalidation, pbounds=hyperparams_ranges,
                                         verbose=2, random_state=42)
        optimizer.maximize(init_points=init_points, n_iter=n_iter, acq=&#39;poi&#39;)
        print(&#39;&#39;)

        # extract and show the results of the optimization
        max_params = {key: None for key in hyperparams_ranges.keys()}
        for key in max_params.keys():
            max_params[key] = optimizer.max[&#39;params&#39;][key]
        print(f&#34;Best target: {optimizer.max[&#39;target&#39;]:.6f}&#34;)
        print(f&#39;Best parameters: {max_params}&#39;)
        self.set_model_params({**self.model_params, **self.__cast_model_params(max_params)})

    def __cast_model_params(self, params):
        &#34;&#34;&#34;
        Check if each model parameter is defined as integer
        and change the parameter dictionary consequently
        Be careful: some libraries like XGBoost do not have
        default parameters with the correct type, it is up to
        the user of this function to make sure that the types
        are correctly initiated in the model before casting
        the params dictionary.

        Parameters
        -----------------------------------------------------
        params: dict
            Hyperparameter values. For
            example: max_depth, learning_rate,
            n_estimators, gamma, min_child_weight,
            subsample, colsample_bytree

        Returns
        -----------------------------------------------------
        out: dict
            Hyperparameter values updated
        &#34;&#34;&#34;
        for key in params.keys():
            if key in self.model.get_params():
                def_val = self.model.get_params()[key]
                params[key] = type(def_val)(round(params[key]) if isinstance(def_val, int) else params[key])
        return params

    def dump_original_model(self, filename, xgb_format=False):
        &#34;&#34;&#34;
        Save the trained model into a pickle
        file. Only for xgboost models it is also given
        the possibility to save them into a .model file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved

        xgb_format : bool
            If True saves the xgboost model into a .model file
        &#34;&#34;&#34;
        if xgb_format is False:
            with open(filename, &#34;wb&#34;) as output_file:
                pickle.dump(self.model, output_file)
        else:
            if self.model_string == &#39;xgboost&#39;:
                self.model.save_model(filename)
            else:
                print(&#34;File not saved: only xgboost models support the .model extension&#34;)

    def dump_model_handler(self, filename):
        &#34;&#34;&#34;
        Save the model handler into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;wb&#34;) as output_file:
            pickle.dump(self, output_file)

    def load_model_handler(self, filename):
        &#34;&#34;&#34;
        Load a model handler saved into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;rb&#34;) as input_file:
            loaded_model = pickle.load(input_file)
            self.model = loaded_model.get_original_model()
            self.training_columns = loaded_model.get_training_columns()
            self.model_params = loaded_model.get_model_params()
            self.model.set_params(**self.model_params)
            self.model_string = loaded_model.get_model_module()
            self._n_classes = loaded_model.get_n_classes()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="hipe4ml.model_handler.ModelHandler.dump_model_handler"><code class="name flex">
<span>def <span class="ident">dump_model_handler</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model handler into a pickle file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file in which the model is saved</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_model_handler(self, filename):
    &#34;&#34;&#34;
    Save the model handler into a pickle file

    Parameters
    -----------------------------------------------------
    filename: str
        Name of the file in which the model is saved
    &#34;&#34;&#34;
    with open(filename, &#34;wb&#34;) as output_file:
        pickle.dump(self, output_file)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.dump_original_model"><code class="name flex">
<span>def <span class="ident">dump_original_model</span></span>(<span>self, filename, xgb_format=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the trained model into a pickle
file. Only for xgboost models it is also given
the possibility to save them into a .model file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file in which the model is saved</dd>
<dt><strong><code>xgb_format</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True saves the xgboost model into a .model file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_original_model(self, filename, xgb_format=False):
    &#34;&#34;&#34;
    Save the trained model into a pickle
    file. Only for xgboost models it is also given
    the possibility to save them into a .model file

    Parameters
    -----------------------------------------------------
    filename: str
        Name of the file in which the model is saved

    xgb_format : bool
        If True saves the xgboost model into a .model file
    &#34;&#34;&#34;
    if xgb_format is False:
        with open(filename, &#34;wb&#34;) as output_file:
            pickle.dump(self.model, output_file)
    else:
        if self.model_string == &#39;xgboost&#39;:
            self.model.save_model(filename)
        else:
            print(&#34;File not saved: only xgboost models support the .model extension&#34;)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.evaluate_hyperparams"><code class="name flex">
<span>def <span class="ident">evaluate_hyperparams</span></span>(<span>self, data, opt_params, scoring, nfold=5, njobs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the cross-validation score for a set of hyper-parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>opt_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Hyperparameters to be optimized. For
example: max_depth, learning_rate,
n_estimators, gamma, min_child_weight,
subsample, colsample_bytree</dd>
<dt><strong><code>scoring</code></strong> :&ensp;<code>string, callable</code> or <code>None</code></dt>
<dd>A string (see sklearn model evaluation documentation:
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a>)
or a scorer callable object / function with signature scorer(estimator, X, y)
which should return only a single value</dd>
<dt><strong><code>nfold</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of folds to perform the cross validation</dd>
<dt><strong><code>njobs</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>Number of CPUs to use to perform computation.
Set to -1 to use all CPUs</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>float</code></dt>
<dd>Cross validation score evaluated using
the ROC AUC metrics</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_hyperparams(self, data, opt_params, scoring, nfold=5, njobs=None):
    &#34;&#34;&#34;
    Calculate the cross-validation score for a set of hyper-parameters

    Parameters
    ------------------------------------------
    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    opt_params: dict
        Hyperparameters to be optimized. For
        example: max_depth, learning_rate,
        n_estimators, gamma, min_child_weight,
        subsample, colsample_bytree

    scoring: string, callable or None
        A string (see sklearn model evaluation documentation:
        https://scikit-learn.org/stable/modules/model_evaluation.html)
        or a scorer callable object / function with signature scorer(estimator, X, y)
        which should return only a single value

    nfold: int
        Number of folds to perform the cross validation

    njobs: int or None
        Number of CPUs to use to perform computation.
        Set to -1 to use all CPUs

    Returns
    ---------------------------------------------------------
    out: float
        Cross validation score evaluated using
        the ROC AUC metrics
    &#34;&#34;&#34;
    opt_params = self.__cast_model_params(opt_params)
    params = {**self.model_params, **opt_params}
    self.model.set_params(**params)
    if self.training_columns is not None:
        return np.mean(cross_val_score(self.model, data[0][self.training_columns], data[1],
                                       cv=nfold, scoring=scoring, n_jobs=njobs))
    return np.mean(cross_val_score(self.model, data[0], data[1], cv=nfold, scoring=scoring, n_jobs=njobs))</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x_train, y_train)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit Model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_train</code></strong> :&ensp;<code>array-like, sparse matrix</code></dt>
<dd>Training data</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>array-like, sparse matrix</code></dt>
<dd>Target data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x_train, y_train):
    &#34;&#34;&#34;
    Fit Model

    Parameters
    ---------------------------
    x_train: array-like, sparse matrix
        Training data

    y_train: array-like, sparse matrix
        Target data
    &#34;&#34;&#34;
    n_classes = len(np.unique(y_train))
    self._n_classes = n_classes
    if self.training_columns is None:
        self.training_columns = list(x_train.columns)

    self.model.fit(x_train[self.training_columns], y_train)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_model_module"><code class="name flex">
<span>def <span class="ident">get_model_module</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the string containing the name
of the model module</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the model module</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_module(self):
    &#34;&#34;&#34;
    Get the string containing the name
    of the model module

    Returns
    ---------------------------
    out: str
        Name of the model module
    &#34;&#34;&#34;
    return self.model_string</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_model_params"><code class="name flex">
<span>def <span class="ident">get_model_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the model (hyper-)parameters</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model hyper-parameter values. For
example (XGBoost): max_depth, learning_rate,
n_estimators, gamma, min_child_weight, &hellip;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_params(self):
    &#34;&#34;&#34;
    Get the model (hyper-)parameters

    Returns
    ------------------------------------
    out: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...
    &#34;&#34;&#34;
    return self.model.get_params()</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_n_classes"><code class="name flex">
<span>def <span class="ident">get_n_classes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the number of classes</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of classes</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_n_classes(self):
    &#34;&#34;&#34;
    Get the number of classes

    Returns
    ---------------------------
    out: int
        Number of classes
    &#34;&#34;&#34;
    return self._n_classes</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_original_model"><code class="name flex">
<span>def <span class="ident">get_original_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the original unwrapped model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>sklearn</code> or <code>XGBoost model</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_original_model(self):
    &#34;&#34;&#34;
    Get the original unwrapped model

    Returns
    ---------------------------
    out: sklearn or XGBoost model
    &#34;&#34;&#34;
    return self.model</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_training_columns"><code class="name flex">
<span>def <span class="ident">get_training_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the features used for the training process</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>list</code></dt>
<dd>Names of the features used for the training.
Example: ['dEdx', 'pT', 'ct']</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_training_columns(self):
    &#34;&#34;&#34;
    Get the features used for the training process

    Returns
    ------------------------------------
    out: list
        Names of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
    &#34;&#34;&#34;

    return self.training_columns</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.load_model_handler"><code class="name flex">
<span>def <span class="ident">load_model_handler</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Load a model handler saved into a pickle file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file in which the model is saved</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_handler(self, filename):
    &#34;&#34;&#34;
    Load a model handler saved into a pickle file

    Parameters
    -----------------------------------------------------
    filename: str
        Name of the file in which the model is saved
    &#34;&#34;&#34;
    with open(filename, &#34;rb&#34;) as input_file:
        loaded_model = pickle.load(input_file)
        self.model = loaded_model.get_original_model()
        self.training_columns = loaded_model.get_training_columns()
        self.model_params = loaded_model.get_model_params()
        self.model.set_params(**self.model_params)
        self.model_string = loaded_model.get_model_module()
        self._n_classes = loaded_model.get_n_classes()</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.optimize_params_bayes"><code class="name flex">
<span>def <span class="ident">optimize_params_bayes</span></span>(<span>self, data, hyperparams_ranges, scoring, nfold=5, init_points=5, n_iter=5, njobs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform Bayesian optimization and update the model hyper-parameters
with the best ones</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>hyperparams_ranges</code></strong> :&ensp;<code>dict</code></dt>
<dd>Hyperparameter ranges(in tuples).
Important: the type of the params must be preserved
when passing the ranges.
For example:
dict={
'max_depth':(10,100)
'learning_rate': (0.01,0.03)
}</dd>
<dt><strong><code>scoring</code></strong> :&ensp;<code>string, callable</code> or <code>None</code></dt>
<dd>A string (see sklearn model evaluation documentation:
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a>)
or a scorer callable object / function with signature scorer(estimator, X, y)
which should return only a single value.
In binary classification 'roc_auc' is suggested.
In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
For more information see
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</a></dd>
<dt><strong><code>nfold</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of folds to calculate the cross validation error</dd>
<dt><strong><code>init_points</code></strong> :&ensp;<code>int</code></dt>
<dd>How many steps of random exploration you want to perform.
Random exploration can help by diversifying the exploration space</dd>
<dt><strong><code>n_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>How many steps for bayesian optimization of the target function.
Bigger n_iter results in better description of thetarget function</dd>
<dt><strong><code>njobs</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>Number of CPUs to perform computation used in the score evaluation
with cross-validation. Set to -1 to use all CPUs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize_params_bayes(self, data, hyperparams_ranges, scoring, nfold=5, init_points=5,
                          n_iter=5, njobs=None):
    &#34;&#34;&#34;
    Perform Bayesian optimization and update the model hyper-parameters
    with the best ones

    Parameters
    ------------------------------------------------------
    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    hyperparams_ranges: dict
        Hyperparameter ranges(in tuples).
        Important: the type of the params must be preserved
        when passing the ranges.
        For example:
        dict={
            &#39;max_depth&#39;:(10,100)
            &#39;learning_rate&#39;: (0.01,0.03)
        }

    scoring: string, callable or None
        A string (see sklearn model evaluation documentation:
        https://scikit-learn.org/stable/modules/model_evaluation.html)
        or a scorer callable object / function with signature scorer(estimator, X, y)
        which should return only a single value.
        In binary classification &#39;roc_auc&#39; is suggested.
        In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
        ‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
        For more information see
        https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

    nfold: int
        Number of folds to calculate the cross validation error

    init_points: int
        How many steps of random exploration you want to perform.
        Random exploration can help by diversifying the exploration space

    n_iter: int
        How many steps for bayesian optimization of the target function.
        Bigger n_iter results in better description of thetarget function

    njobs: int or None
        Number of CPUs to perform computation used in the score evaluation
        with cross-validation. Set to -1 to use all CPUs
    &#34;&#34;&#34;
    n_classes = len(np.unique(data[1]))
    self._n_classes = n_classes
    if self.training_columns is None:
        self.training_columns = list(data[0].columns)

    start_params = {}
    for key in hyperparams_ranges:
        start_params[key] = hyperparams_ranges[key][0]
    self.set_model_params({**self.model_params, **start_params})

    # just an helper function
    def hyperparams_crossvalidation(**kwargs):
        return self.evaluate_hyperparams(data, kwargs, scoring, nfold, njobs)
    print(&#39;&#39;)

    optimizer = BayesianOptimization(f=hyperparams_crossvalidation, pbounds=hyperparams_ranges,
                                     verbose=2, random_state=42)
    optimizer.maximize(init_points=init_points, n_iter=n_iter, acq=&#39;poi&#39;)
    print(&#39;&#39;)

    # extract and show the results of the optimization
    max_params = {key: None for key in hyperparams_ranges.keys()}
    for key in max_params.keys():
        max_params[key] = optimizer.max[&#39;params&#39;][key]
    print(f&#34;Best target: {optimizer.max[&#39;target&#39;]:.6f}&#34;)
    print(f&#39;Best parameters: {max_params}&#39;)
    self.set_model_params({**self.model_params, **self.__cast_model_params(max_params)})</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x_test, output_margin=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Return model prediction for the array x_test</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_test</code></strong> :&ensp;<code>hipe4ml tree_handler, array-like, sparse matrix</code></dt>
<dd>The input sample.</dd>
<dt><strong><code>output_margin</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to output the raw untransformed margin value. If False model
probabilities are returned</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Model predictions</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x_test, output_margin=True):
    &#34;&#34;&#34;
    Return model prediction for the array x_test

    Parameters
    --------------------------------------
    x_test: hipe4ml tree_handler, array-like, sparse matrix
        The input sample.

    output_margin: bool
        Whether to output the raw untransformed margin value. If False model
        probabilities are returned

    Returns
    ---------------------------------------
    out: numpy array
        Model predictions
    &#34;&#34;&#34;
    if isinstance(x_test, hipe4ml.tree_handler.TreeHandler):
        x_test = x_test.get_data_frame()

    x_test = x_test[self.training_columns]

    if output_margin:
        if self.model_string == &#39;xgboost&#39;:
            pred = self.model.predict(x_test, True)
        if self.model_string == &#39;sklearn&#39;:
            pred = self.model.decision_function(x_test).ravel()
    else:
        pred = self.model.predict_proba(x_test)
        # in case of binary classification return only the scores of
        # the signal class
        if pred.shape[1] &lt;= 2:
            pred = pred[:, 1]

    return pred</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.set_model_params"><code class="name flex">
<span>def <span class="ident">set_model_params</span></span>(<span>self, model_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the model (hyper-)parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model hyper-parameter values. For
example (XGBoost): max_depth, learning_rate,
n_estimators, gamma, min_child_weight, &hellip;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_model_params(self, model_params):
    &#34;&#34;&#34;
    Set the model (hyper-)parameters

    Parameters
    ------------------------------------
    model_params: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...
    &#34;&#34;&#34;
    self.model_params = model_params
    self.model.set_params(**self.model_params)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.set_training_columns"><code class="name flex">
<span>def <span class="ident">set_training_columns</span></span>(<span>self, training_columns)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the features used for the training process</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>training_columns</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the name of the features used for the training.
Example: ['dEdx', 'pT', 'ct']</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_training_columns(self, training_columns):
    &#34;&#34;&#34;
    Set the features used for the training process

    Parameters
    ------------------------------------
    training_columns: list
        Contains the name of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
    &#34;&#34;&#34;
    self.training_columns = training_columns</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.train_test_model"><code class="name flex">
<span>def <span class="ident">train_test_model</span></span>(<span>self, data, return_prediction=False, output_margin=False, average='macro', multi_class_opt='raise')</span>
</code></dt>
<dd>
<div class="desc"><p>Perform the training and the testing of the model. The model performance is estimated
using the ROC AUC metric</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>return_prediction</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True Model predictions on the test set are
returned</dd>
<dt><strong><code>output_margin</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to output the raw untransformed margin value. If False model
probabilities are returned</dd>
<dt><strong><code>average</code></strong> :&ensp;<code>string</code></dt>
<dd>Option for the average of ROC AUC scores used only in case of multi-classification.
You can choose between 'macro' and 'weighted'. For more information see
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score</a></dd>
<dt><strong><code>multi_class_opt</code></strong> :&ensp;<code>string</code></dt>
<dd>Option to compute ROC AUC scores used only in case of multi-classification.
The one-vs-one 'ovo' and one-vs-rest 'ovr' approaches are available</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code> or <code>None</code></dt>
<dd>If return_prediction==True, Model predictions on the test set are
returned</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_test_model(self, data, return_prediction=False, output_margin=False, average=&#39;macro&#39;,
                     multi_class_opt=&#39;raise&#39;):
    &#34;&#34;&#34;
    Perform the training and the testing of the model. The model performance is estimated
    using the ROC AUC metric

    Parameters
    ----------------------------------------------
    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    return_prediction: bool
        If True Model predictions on the test set are
        returned

    output_margin: bool
        Whether to output the raw untransformed margin value. If False model
        probabilities are returned

    average: string
        Option for the average of ROC AUC scores used only in case of multi-classification.
        You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

    multi_class_opt: string
        Option to compute ROC AUC scores used only in case of multi-classification.
        The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

    Returns
    ---------------------------------------
    out: numpy array or None
        If return_prediction==True, Model predictions on the test set are
        returned

    &#34;&#34;&#34;

    # get number of classes
    n_classes = len(np.unique(data[1]))
    self._n_classes = n_classes
    print(&#39;Number of detected classes:&#39;, n_classes)

    # final training with the optimized hyperparams
    print(&#39;Training the final model: ...&#39;, end=&#39;\r&#39;)
    self.fit(data[0], data[1])
    print(&#39;Training the final model: Done!&#39;)
    print(&#39;Testing the model: ...&#39;, end=&#39;\r&#39;)
    y_pred = self.predict(data[2], output_margin=output_margin)
    roc_score = roc_auc_score(
        data[3], y_pred, average=average, multi_class=multi_class_opt)
    print(&#39;Testing the model: Done!&#39;)

    print(f&#39;ROC_AUC_score: {roc_score:.6f}&#39;)
    print(&#39;==============================&#39;)
    if return_prediction:
        return y_pred
    return None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hipe4ml" href="index.html">hipe4ml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hipe4ml.model_handler.ModelHandler" href="#hipe4ml.model_handler.ModelHandler">ModelHandler</a></code></h4>
<ul class="">
<li><code><a title="hipe4ml.model_handler.ModelHandler.dump_model_handler" href="#hipe4ml.model_handler.ModelHandler.dump_model_handler">dump_model_handler</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.dump_original_model" href="#hipe4ml.model_handler.ModelHandler.dump_original_model">dump_original_model</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.evaluate_hyperparams" href="#hipe4ml.model_handler.ModelHandler.evaluate_hyperparams">evaluate_hyperparams</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.fit" href="#hipe4ml.model_handler.ModelHandler.fit">fit</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_model_module" href="#hipe4ml.model_handler.ModelHandler.get_model_module">get_model_module</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_model_params" href="#hipe4ml.model_handler.ModelHandler.get_model_params">get_model_params</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_n_classes" href="#hipe4ml.model_handler.ModelHandler.get_n_classes">get_n_classes</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_original_model" href="#hipe4ml.model_handler.ModelHandler.get_original_model">get_original_model</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_training_columns" href="#hipe4ml.model_handler.ModelHandler.get_training_columns">get_training_columns</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.load_model_handler" href="#hipe4ml.model_handler.ModelHandler.load_model_handler">load_model_handler</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.optimize_params_bayes" href="#hipe4ml.model_handler.ModelHandler.optimize_params_bayes">optimize_params_bayes</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.predict" href="#hipe4ml.model_handler.ModelHandler.predict">predict</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.set_model_params" href="#hipe4ml.model_handler.ModelHandler.set_model_params">set_model_params</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.set_training_columns" href="#hipe4ml.model_handler.ModelHandler.set_training_columns">set_training_columns</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.train_test_model" href="#hipe4ml.model_handler.ModelHandler.train_test_model">train_test_model</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>