<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>hipe4ml.plot_utils API documentation</title>
<meta name="description" content="Module containing the plot utils. Each function returns a matplotlib object" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hipe4ml.plot_utils</code></h1>
</header>
<section id="section-intro">
<p>Module containing the plot utils. Each function returns a matplotlib object</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Module containing the plot utils. Each function returns a matplotlib object
&#34;&#34;&#34;

from itertools import combinations

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shap
from mpl_toolkits.axes_grid1 import ImageGrid
from scipy.special import softmax
from sklearn.metrics import (auc, average_precision_score, mean_squared_error,
                             precision_recall_curve, roc_auc_score, roc_curve)
from sklearn.preprocessing import label_binarize

import hipe4ml.tree_handler


def _plot_output(df_train, df_test, lims, bins, label, color, kwds):
    &#34;&#34;&#34;
    Utility function for plot_output_train_test
    &#34;&#34;&#34;
    plt.hist(df_train, color=color, alpha=0.5, range=lims, bins=bins,
             histtype=&#39;stepfilled&#39;, label=f&#39;{label} pdf Training Set&#39;, **kwds)

    if &#39;density&#39; in kwds and kwds[&#39;density&#39;]:
        hist, bins = np.histogram(df_test, bins=bins, range=lims, density=True)
        scale = len(df_test) / sum(hist)
        err = np.sqrt(hist * scale) / scale
    else:
        hist, bins = np.histogram(df_test, bins=bins, range=lims)
        scale = len(df_train) / len(df_test)
        err = np.sqrt(hist) * scale
        hist = hist * scale

    center = (bins[:-1] + bins[1:]) / 2
    plt.errorbar(center, hist, yerr=err, fmt=&#39;o&#39;,
                 c=color, label=f&#39;{label} pdf Test Set&#39;)


def plot_output_train_test(model, data, bins=80, output_margin=True, labels=None, logscale=False, **kwds):
    &#34;&#34;&#34;
    Plot the model output distributions for each class and output
    both for training and test set.

    Parameters
    ----------------------------------------
    model: hipe4ml model handler

    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    bins: int or sequence of scalars or str
        If bins is an int, it defines the number of equal-width
        bins in the given range (10, by default). If bins is a
        sequence, it defines a monotonically increasing array of
        bin edges, including the rightmost edge, allowing for
        non-uniform bin widths.
        If bins is a string, it defines the method used to
        calculate the optimal bin width, as defined by
        np.histogram_bin_edges:
        https://docs.scipy.org/doc/numpy/reference/generated
        /numpy.histogram_bin_edges.html#numpy.histogram_bin_edges

    output_margin: bool
        Whether to output the raw untransformed margin value.

    labels: list
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    logscale: bool
        Whether to plot the y axis in log scale

    **kwds
        Extra arguments are passed on to plt.hist()

    Returns
    ----------------------------------------
    out: matplotlib.figure.Figure or list of them
        Model output distributions for each class
    &#34;&#34;&#34;
    class_labels = np.unique(data[1])
    n_classes = len(class_labels)

    prediction = []
    for xxx, yyy in ((data[0], data[1]), (data[2], data[3])):
        for class_lab in class_labels:
            prediction.append(model.predict(
                xxx[yyy == class_lab], output_margin))

    low = min(np.min(d) for d in prediction)
    high = max(np.max(d) for d in prediction)
    low_high = (low, high)

    # only one figure in case of binary classification
    if n_classes &lt;= 2:
        res = plt.figure()
        labels = [&#39;Signal&#39;, &#39;Background&#39;] if labels is None else labels
        colors = [&#39;b&#39;, &#39;r&#39;]
        for i_class, (label, color) in enumerate(zip(labels, colors)):
            _plot_output(
                prediction[i_class], prediction[i_class+2], low_high, bins, label, color, kwds)
        if logscale:
            plt.yscale(&#39;log&#39;)
        plt.xlabel(&#39;BDT output&#39;, fontsize=13, ha=&#39;right&#39;, position=(1, 20))
        plt.ylabel(&#39;Counts (arb. units)&#39;, fontsize=13,
                   horizontalalignment=&#39;left&#39;)
        plt.legend(frameon=False, fontsize=12, loc=&#39;best&#39;)

    # n figures in case of multi-classification with n classes
    else:
        res = []
        labels = [
            f&#39;class{class_lab}&#39; for class_lab in class_labels] if labels is None else labels
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        colors = [cmap(i_class) for i_class in range(len(labels))]
        for output, out_label in zip(class_labels, labels):
            res.append(plt.figure())
            for i_class, (label, color) in enumerate(zip(labels, colors)):
                _plot_output(prediction[i_class][:, output], prediction[i_class+n_classes][:, output], low_high, bins,
                             label, color, kwds)
            if logscale:
                plt.yscale(&#39;log&#39;)
            plt.xlabel(f&#39;BDT output for {out_label}&#39;,
                       fontsize=13, ha=&#39;right&#39;, position=(1, 20))
            plt.ylabel(&#39;Counts (arb. units)&#39;, fontsize=13,
                       horizontalalignment=&#39;left&#39;)
            plt.legend(frameon=False, fontsize=12, loc=&#39;best&#39;)

    return res

# flake8: noqa: C901
def plot_distr(data_list, column=None, bins=50, labels=None, colors=None, **kwds):  # pylint: disable=too-many-branches
    &#34;&#34;&#34;
    Draw histograms comparing the distributions of each class.

    Parameters
    -----------------------------------------
    data_list: TreeHandler, pandas.Dataframe or list of them
        Contains a TreeHandler or a dataframe for each class

    column: str or list of them
        Contains the name of the features you want to plot
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]. If None all the features
        are selected

    bins: int or sequence of scalars or str
        If bins is an int, it defines the number of equal-width
        bins in the given range (10, by default). If bins is a
        sequence, it defines a monotonically increasing array of
        bin edges, including the rightmost edge, allowing for
        non-uniform bin widths.

    labels: str or list of them
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    colors: str or list of them
        List of the colors to be used to fill the histograms

    **kwds:
        extra arguments are passed on to pandas.DataFrame.hist():
        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html

    Returns
    -----------------------------------------
    out: numpy array of matplotlib.axes.AxesSubplot
        Distributions of the features for each class
    &#34;&#34;&#34;
    list_of_df = []
    if not isinstance(data_list, list):
        data_list = [data_list]
        labels = [labels]
        colors = [colors]
    if isinstance(data_list[0], hipe4ml.tree_handler.TreeHandler):
        for handl in data_list:
            list_of_df.append(handl.get_data_frame())
    else:
        list_of_df = data_list

    if column is not None:
        if not isinstance(column, (list, np.ndarray, pd.Index)):
            column = [column]

    else:
        column = list(list_of_df[0].columns)

    if labels is None:
        labels = [f&#39;class{i_class}&#39; for i_class, _ in enumerate(list_of_df)]

    if colors is None:
        colors = [None for i_class, _ in enumerate(list_of_df)]

    for i_class, (dfm, lab, col) in enumerate(zip(list_of_df, labels, colors)):
        if i_class == 0:
            axes = dfm.hist(column=column, bins=bins, label=lab, color=col, **kwds)
            axes = axes.flatten()
            axes = axes[:len(column)]
        else:
            dfm.hist(ax=axes, column=column, bins=bins, label=lab, color=col, **kwds)
    for axs in axes:
        axs.set_ylabel(&#39;Counts&#39;)
    axes[-1].legend(loc=&#39;best&#39;)
    if len(axes) == 1:
        axes = axes[0]
    return axes


def plot_corr(data_list, columns, labels=None, **kwds):
    &#34;&#34;&#34;
    Calculate pairwise correlation between features for
    each class (e.g. signal and background in case of binary
    classification)

    Parameters
    -----------------------------------------------
    data_list: list
        Contains dataframes for each class

    columns: list
        Contains the name of the features you want to plot
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    labels: list
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    **kwds: extra arguments are passed on to DataFrame.corr()

    Returns
    ------------------------------------------------
    out: matplotlib.figure.Figure or list of them
        Correlations between the features for each class
    &#34;&#34;&#34;
    list_of_df = []
    if isinstance(data_list[0], hipe4ml.tree_handler.TreeHandler):
        for handl in data_list:
            list_of_df.append(handl.get_data_frame())
    else:
        list_of_df = data_list
    corr_mat = []
    for dfm in list_of_df:
        dfm = dfm[columns]
        corr_mat.append(dfm.corr(**kwds))

    if labels is None:
        labels = []
        if len(corr_mat) != 2:
            for i_mat, _ in enumerate(corr_mat):
                labels.append(f&#39;class{i_mat}&#39;)
        else:
            labels.append(&#39;Signal&#39;)
            labels.append(&#39;Background&#39;)

    res = []
    for mat, lab in zip(corr_mat, labels):
        if len(corr_mat) &lt; 2:
            res = plt.figure(figsize=(8, 7))
            grid = ImageGrid(res, 111, axes_pad=0.15, nrows_ncols=(1, 1), share_all=True,
                             cbar_location=&#39;right&#39;, cbar_mode=&#39;single&#39;, cbar_size=&#39;7%&#39;, cbar_pad=0.15)
        else:
            res.append(plt.figure(figsize=(8, 7)))
            grid = ImageGrid(res[-1], 111, axes_pad=0.15, nrows_ncols=(1, 1), share_all=True,
                             cbar_location=&#39;right&#39;, cbar_mode=&#39;single&#39;, cbar_size=&#39;7%&#39;, cbar_pad=0.15)

        opts = {&#39;cmap&#39;: plt.get_cmap(
            &#39;coolwarm&#39;), &#39;vmin&#39;: -1, &#39;vmax&#39;: +1, &#39;snap&#39;: True}

        axs = grid[0]
        heatmap = axs.pcolor(mat, **opts)
        axs.set_title(lab, fontsize=14, fontweight=&#39;bold&#39;)

        lab = mat.columns.values

        # shift location of ticks to center of the bins
        axs.set_xticks(np.arange(len(lab)), minor=False)
        axs.set_yticks(np.arange(len(lab)), minor=False)
        axs.set_xticklabels(lab, minor=False, ha=&#39;left&#39;,
                            rotation=90, fontsize=10)
        axs.set_yticklabels(lab, minor=False, va=&#39;bottom&#39;, fontsize=10)
        axs.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, direction=&#34;in&#34;)

        for tick in axs.xaxis.get_minor_ticks():
            tick.tick1line.set_markersize(0)
            tick.tick2line.set_markersize(0)
            tick.label1.set_horizontalalignment(&#39;center&#39;)

        plt.colorbar(heatmap, axs.cax)

    return res


def plot_bdt_eff(threshold, eff_sig):
    &#34;&#34;&#34;
    Plot the model efficiency calculated with the function
    bdt_efficiency_array() in analysis_utils

    Parameters
    -----------------------------------
    threshold: array
        Score threshold array

    eff_sig: array
        model efficiency array

    Returns
    -----------------------------------
    out: matplotlib.figure.Figure
        Plot containing model efficiency as a
        function of the threshold score
    &#34;&#34;&#34;
    res = plt.figure()
    plt.plot(threshold, eff_sig, &#39;r.&#39;, label=&#39;Signal efficiency&#39;)
    plt.legend()
    plt.xlabel(&#39;BDT Score&#39;)
    plt.ylabel(&#39;Efficiency&#39;)
    plt.title(&#39;Efficiency vs Score&#39;)
    plt.grid()
    return res


def _plot_roc_ovr(y_truth, y_score, n_classes, labels, average):
    &#34;&#34;&#34;
    Utility function for plot_roc in the multi-class case. Calculate and plot the
    ROC curves with the one-vs-rest approach
    &#34;&#34;&#34;
    cmap = plt.cm.get_cmap(&#39;tab10&#39;)
    # convert multi-class labels to multi-labels to obtain roc curves
    y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
    for i_class, lab in enumerate(labels):
        fpr, tpr, _ = roc_curve(y_truth_multi[:, i_class], y_score[:, i_class])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, c=cmap(i_class),
                 label=f&#39;{lab} Vs Rest (AUC = {roc_auc:.4f})&#39;)
    # compute global ROC AUC
    global_roc_auc = roc_auc_score(
        y_truth, y_score, average=average, multi_class=&#39;ovr&#39;)
    plt.plot([], [], &#39; &#39;, label=f&#39;Average OvR ROC AUC: {global_roc_auc:.4f}&#39;)


def _plot_roc_ovo(y_truth, y_score, n_classes, labels, average):
    &#34;&#34;&#34;
    Utility function for plot_roc in the multi-class case. Calculate and plot the
    ROC curves with the one-vs-one approach
    &#34;&#34;&#34;
    cmap = plt.cm.get_cmap(&#39;tab10&#39;)
    for i_comb, (aaa, bbb) in enumerate(combinations(range(n_classes), 2)):
        a_mask = y_truth == aaa
        b_mask = y_truth == bbb
        ab_mask = np.logical_or(a_mask, b_mask)
        a_true = a_mask[ab_mask]
        b_true = b_mask[ab_mask]
        fpr_a, tpr_a, _ = roc_curve(a_true, y_score[ab_mask, aaa])
        roc_auc_a = auc(fpr_a, tpr_a)
        fpr_b, tpr_b, _ = roc_curve(b_true, y_score[ab_mask, bbb])
        roc_auc_b = auc(fpr_b, tpr_b)
        plt.plot(fpr_a, tpr_a, lw=1, c=cmap(i_comb),
                 label=f&#39;{labels[aaa]} Vs {labels[bbb]} (AUC = {roc_auc_a:.4f})&#39;)
        plt.plot(fpr_b, tpr_b, lw=1, c=cmap(i_comb), alpha=0.6,
                 label=f&#39;{labels[bbb]} Vs {labels[aaa]} (AUC = {roc_auc_b:.4f})&#39;)
    # compute global ROC AUC
    global_roc_auc = roc_auc_score(
        y_truth, y_score, average=average, multi_class=&#39;ovo&#39;)
    plt.plot([], [], &#39; &#39;, label=f&#39;Average OvO ROC AUC: {global_roc_auc:.4f}&#39;)


def plot_roc(y_truth, y_score, pos_label=None, labels=None, average=&#39;macro&#39;, multi_class_opt=&#39;raise&#39;):
    &#34;&#34;&#34;
    Calculate and plot the roc curve

    Parameters
    -------------------------------------
    y_truth: array
        True labels for the belonging class. If labels are not
        {0, 1} in binary classification, then pos_label should
        be explicitly given. In multi-classification labels must
        be {0, 1, ..., N}

    y_score: array
        Target scores, can either be probability estimates or
        non-thresholded measure of decisions (as returned
        by “decision_function” on some classifiers).

    pos_label: int or str
        The label of the positive class. Only available in binary
        classification. When pos_label=None, if y_true is in {0, 1},
        pos_label is set to 1, otherwise an error will be raised.

    labels: list
        Contains the labels to be displayed in the legend, used only in case of
        multi-classification. They must be in the same order as the y_score columns.
        If None the labels are class1, class2, ..., classN

    average: string
        Option for the average of ROC AUC scores used only in case of multi-classification.
        You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

    multi_class_opt: string
        Option to compute ROC curves used only in case of multi-classification.
        The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the roc curves
    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    res = plt.figure()
    if n_classes &lt;= 2:
        # binary case
        fpr, tpr, _ = roc_curve(y_truth, y_score, pos_label=pos_label)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, label=f&#39;ROC (AUC = {roc_auc:.4f})&#39;)

    else:
        # multi-class case
        if (labels is None) or (labels and len(labels) != n_classes):
            labels = [f&#39;class{i_class}&#39; for i_class in range(n_classes)]
        if multi_class_opt not in [&#39;ovo&#39;, &#39;ovr&#39;]:
            print(&#39;ERROR: if n_class &gt; 2 multi_class_opt must be ovo or ovr&#39;)
            return res

        # check to have numpy arrays
        if not isinstance(y_truth, np.ndarray):
            y_truth = np.array(y_truth)
        if not isinstance(y_score, np.ndarray):
            y_score = np.array(y_score)

        # if y_score contains raw outputs transform them to probabilities
        if not np.allclose(1, y_score.sum(axis=1)):
            y_score = softmax(y_score, axis=1)

        # one-vs-rest case
        if multi_class_opt == &#39;ovr&#39;:
            _plot_roc_ovr(y_truth, y_score, n_classes, labels, average)
        # one-vs-one case
        if multi_class_opt == &#39;ovo&#39;:
            _plot_roc_ovo(y_truth, y_score, n_classes, labels, average)

    plt.plot([0, 1], [0, 1], &#39;-.&#39;, color=(0.6, 0.6, 0.6), label=&#39;Luck&#39;)
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()

    return res


def plot_roc_train_test(y_truth_test, y_score_test, y_truth_train, y_score_train, pos_label=None, labels=None,
                        average=&#39;macro&#39;, multi_class_opt=&#39;raise&#39;):
    &#34;&#34;&#34;
    Calculate and plot the roc curve for test and train sets

    Parameters
    -------------------------------------
    y_truth_test: array
        True labels for the belonging class of the test set. If labels
        are not {0, 1} in binary classification, then pos_label should
        be explicitly given. In multi-classification labels must be
        {0, 1, ..., N}

    y_score_test: array
        Target scores for the test set, can either be probability
        estimates or non-thresholded measure of decisions (as returned
        by “decision_function” on some classifiers).

    y_truth_train: array
        True labels for the belonging class of the train set. If labels
        are not {0, 1} in binary classification, then pos_label should
        be explicitly given. In multi-classification labels must be
        {0, 1, ..., N}

    y_score_train: array
        Target scores for the train set, can either be probability
        estimates or non-thresholded measure of decisions (as returned
        by “decision_function” on some classifiers).

    pos_label: int or str
        The label of the positive class. Only available in binary
        classification. When pos_label=None, if y_true is in {0, 1},
        pos_label is set to 1, otherwise an error will be raised.

    labels: list
        Contains the labels to be displayed in the legend, used only in case of
        multi-classification. They must be in the same order as the y_score columns.
        If None the labels are class1, class2, ..., classN

    average: string
        Option for the average of ROC AUC scores used only in case of multi-classification.
        You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

    multi_class_opt: string
        Option to compute ROC curves used only in case of multi-classification.
        The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the roc curves
    &#34;&#34;&#34;
    # call plot_roc for both train and test sets
    fig_test = plot_roc(y_truth_test, y_score_test,
                        pos_label, labels, average, multi_class_opt)
    fig_train = plot_roc(y_truth_train, y_score_train,
                         pos_label, labels, average, multi_class_opt)
    axes_test = fig_test.get_axes()[0]
    axes_train = fig_train.get_axes()[0]

    # plot results together
    res = plt.figure()
    for roc_test, roc_train in zip(axes_test.lines, axes_train.lines):
        test_label = roc_test.get_label()
        train_label = roc_train.get_label()
        if &#39;Luck&#39; in [test_label, train_label]:
            continue

        plt.plot(roc_test.get_xdata(), roc_test.get_ydata(), lw=roc_test.get_lw(), c=roc_test.get_c(),
                 alpha=roc_test.get_alpha(), marker=roc_test.get_marker(), linestyle=roc_test.get_linestyle(),
                 label=f&#39;Test -&gt; {test_label}&#39;)

        linestyle = roc_train.get_linestyle()
        if linestyle in &#39;-&#39;:
            linestyle = &#39;--&#39;
        plt.plot(roc_train.get_xdata(), roc_train.get_ydata(), lw=roc_train.get_lw(), c=roc_train.get_c(),
                 alpha=roc_train.get_alpha(), marker=roc_train.get_marker(), linestyle=linestyle,
                 label=f&#39;Train -&gt; {train_label}&#39;)

    plt.plot([0, 1], [0, 1], &#39;-.&#39;, color=(0.6, 0.6, 0.6), label=&#39;Luck&#39;)
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()

    return res


def plot_feature_imp(df_in, y_truth, model, labels=None, n_sample=10000, approximate=True):
    &#34;&#34;&#34;
    Calculate the feature importance using the shap algorithm for
    each feature. The calculation is performed on a subsample of the
    input training/test set

    Parameters
    -------------------------------------------
    df_in: Pandas dataframe
        Training or test set dataframe

    y_truth: array
        Training or test set label

    model: hipe4ml model_handler
        Trained model

    labels: list
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    n_sample: int
        Number of candidates employed to fill
        the shap plots. If larger than the number of
        candidates in each class, minimum number of candidates
        in a given class used instead

    approximate: bool
        Run fast and approximat roughly the SHAP values. For more information
        see https://shap.readthedocs.io/en/latest/#shap.TreeExplainer.shap_values

    Returns
    -------------------------------------------
    out: List of matplotlib.figure.Figure
        Plots with shap feature importance. The first ones are the shap violin plots
        computed for each class(in case of binary classification only one plot is returned).
        The last plot of the list is built by taking the mean absolute value of the SHAP
        values for each feature to get a standard bar plot (stacked bars are produced for
        multi-class outputs)

    &#34;&#34;&#34;
    class_labels, class_counts = np.unique(y_truth, return_counts=True)
    n_classes = len(class_labels)
    for class_count in class_counts:
        if n_sample &gt; class_count:
            n_sample = class_count

    subs = []
    for class_lab in class_labels:
        subs.append(df_in[y_truth == class_lab].sample(n_sample))

    df_subs = pd.concat(subs)
    df_subs = df_subs[model.get_training_columns()]
    explainer = shap.TreeExplainer(model.get_original_model())
    shap_values = explainer.shap_values(df_subs, approximate=approximate)
    res = []

    if n_classes &lt;= 2:
        res.append(plt.figure(figsize=(18, 9)))
        shap.summary_plot(shap_values, df_subs, plot_size=(
            18, 9), class_names=labels, show=False)
    else:
        for i_class in range(n_classes):
            res.append(plt.figure(figsize=(18, 9)))
            shap.summary_plot(shap_values[i_class], df_subs, plot_size=(
                18, 9), class_names=labels, show=False)

    res.append(plt.figure(figsize=(18, 9)))
    shap.summary_plot(shap_values, df_subs, plot_type=&#39;bar&#39;, plot_size=(
        18, 9), class_names=labels, show=False)

    return res


def plot_precision_recall(y_truth, y_score, labels=None, pos_label=None):
    &#34;&#34;&#34; Plot precision recall curve

    Parameters
    -------------------------------------
    y_truth: array
        True labels for the belonging class. If labels are not
        {0, 1, ..., N}, then pos_label should be explicitly given.

    y_score: array
        Estimated probabilities or decision function.

    pos_label : int or str
        The label of the positive class. When pos_label=None,
        if y_true is in {0, 1, ..., N}, pos_label is set to 1,
        otherwise an error will be raised.

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the precision recall curves
    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(f&#39;class{i_class}&#39;)

    res = plt.figure()
    if n_classes &lt;= 2:
        precision, recall, _ = precision_recall_curve(
            y_truth, y_score, pos_label=pos_label)
        plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;)
        plt.fill_between(recall, precision, alpha=0.2, color=&#39;b&#39;, step=&#39;post&#39;)
        average_precision = average_precision_score(y_truth, y_score)
        plt.title(
            f&#39;2-class Precision-Recall curve: AP={average_precision:0.2f}&#39;)
    else:
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        precision, recall = (dict() for i_dict in range(2))
        # convert multi-class labels to multi-labels to obtain a curve for each class
        y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
        for clas, lab in enumerate(labels):
            precision[clas], recall[clas], _ = precision_recall_curve(
                y_truth_multi[:, clas], y_score[:, clas], pos_label=pos_label)
            plt.step(recall[clas], precision[clas], color=cmap(clas), lw=1, where=&#39;post&#39;,
                     label=lab)
        # compute also micro average
        precision[&#39;micro&#39;], recall[&#39;micro&#39;], _ = precision_recall_curve(
            y_truth_multi.ravel(), y_score.ravel())
        plt.step(recall[&#39;micro&#39;], precision[&#39;micro&#39;], color=&#39;black&#39;, where=&#39;post&#39;,
                 linestyle=&#39;--&#39;, lw=1, label=&#39;average&#39;)
        average_precision = average_precision_score(
            y_truth_multi, y_score, average=&#39;micro&#39;)
        plt.title(
            f&#39;Average precision score, micro-averaged over all classes: {average_precision:0.2f}&#39;)

    plt.xlabel(&#39;Recall&#39;)
    plt.ylabel(&#39;Precision&#39;)
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    if n_classes &gt; 2:
        plt.legend(loc=&#39;lower left&#39;)
        plt.grid()
    return res


def plot_learning_curves(model, data, n_points=10):
    &#34;&#34;&#34; Plot learning curves

    Parameters
    -------------------------------------
    model: hipe4ml model_handler

    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    n_points: int
        Number of points used to sample the learning curves

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the learning curves
    &#34;&#34;&#34;

    res = plt.figure()
    train_errors, test_errors = [], []
    min_cand = 100
    max_cand = len(data[0])
    step = int((max_cand-min_cand)/n_points)
    array_n_cand = np.arange(start=min_cand, stop=max_cand, step=step)
    for n_cand in array_n_cand:
        model.fit(data[0][:n_cand], data[1][:n_cand])
        y_train_predict = model.predict(data[0][:n_cand], output_margin=False)
        y_test_predict = model.predict(data[2], output_margin=False)
        train_errors.append(mean_squared_error(
            y_train_predict, data[1][:n_cand], multioutput=&#39;uniform_average&#39;))
        test_errors.append(mean_squared_error(
            y_test_predict, data[3], multioutput=&#39;uniform_average&#39;))
    plt.plot(array_n_cand, np.sqrt(train_errors), &#39;r&#39;, lw=1, label=&#39;Train&#39;)
    plt.plot(array_n_cand, np.sqrt(test_errors), &#39;b&#39;, lw=1, label=&#39;Test&#39;)
    plt.ylim([0, np.amax(np.sqrt(test_errors))*2])
    plt.xlabel(&#39;Training set size&#39;)
    plt.ylabel(&#39;RMSE&#39;)
    plt.grid()
    plt.legend(loc=&#39;best&#39;)

    return res</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hipe4ml.plot_utils.plot_bdt_eff"><code class="name flex">
<span>def <span class="ident">plot_bdt_eff</span></span>(<span>threshold, eff_sig)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the model efficiency calculated with the function
bdt_efficiency_array() in analysis_utils</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>array</code></dt>
<dd>Score threshold array</dd>
<dt><strong><code>eff_sig</code></strong> :&ensp;<code>array</code></dt>
<dd>model efficiency array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>Plot containing model efficiency as a
function of the threshold score</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_bdt_eff(threshold, eff_sig):
    &#34;&#34;&#34;
    Plot the model efficiency calculated with the function
    bdt_efficiency_array() in analysis_utils

    Parameters
    -----------------------------------
    threshold: array
        Score threshold array

    eff_sig: array
        model efficiency array

    Returns
    -----------------------------------
    out: matplotlib.figure.Figure
        Plot containing model efficiency as a
        function of the threshold score
    &#34;&#34;&#34;
    res = plt.figure()
    plt.plot(threshold, eff_sig, &#39;r.&#39;, label=&#39;Signal efficiency&#39;)
    plt.legend()
    plt.xlabel(&#39;BDT Score&#39;)
    plt.ylabel(&#39;Efficiency&#39;)
    plt.title(&#39;Efficiency vs Score&#39;)
    plt.grid()
    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_corr"><code class="name flex">
<span>def <span class="ident">plot_corr</span></span>(<span>data_list, columns, labels=None, **kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate pairwise correlation between features for
each class (e.g. signal and background in case of binary
classification)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains dataframes for each class</dd>
<dt><strong><code>columns</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the name of the features you want to plot
Example: ['dEdx', 'pT', 'ct']</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</dd>
<dt><strong><code>**kwds</code></strong> :&ensp;<code>extra arguments are passed on to DataFrame.corr()</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code> or <code>list</code> of <code>them</code></dt>
<dd>Correlations between the features for each class</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_corr(data_list, columns, labels=None, **kwds):
    &#34;&#34;&#34;
    Calculate pairwise correlation between features for
    each class (e.g. signal and background in case of binary
    classification)

    Parameters
    -----------------------------------------------
    data_list: list
        Contains dataframes for each class

    columns: list
        Contains the name of the features you want to plot
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    labels: list
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    **kwds: extra arguments are passed on to DataFrame.corr()

    Returns
    ------------------------------------------------
    out: matplotlib.figure.Figure or list of them
        Correlations between the features for each class
    &#34;&#34;&#34;
    list_of_df = []
    if isinstance(data_list[0], hipe4ml.tree_handler.TreeHandler):
        for handl in data_list:
            list_of_df.append(handl.get_data_frame())
    else:
        list_of_df = data_list
    corr_mat = []
    for dfm in list_of_df:
        dfm = dfm[columns]
        corr_mat.append(dfm.corr(**kwds))

    if labels is None:
        labels = []
        if len(corr_mat) != 2:
            for i_mat, _ in enumerate(corr_mat):
                labels.append(f&#39;class{i_mat}&#39;)
        else:
            labels.append(&#39;Signal&#39;)
            labels.append(&#39;Background&#39;)

    res = []
    for mat, lab in zip(corr_mat, labels):
        if len(corr_mat) &lt; 2:
            res = plt.figure(figsize=(8, 7))
            grid = ImageGrid(res, 111, axes_pad=0.15, nrows_ncols=(1, 1), share_all=True,
                             cbar_location=&#39;right&#39;, cbar_mode=&#39;single&#39;, cbar_size=&#39;7%&#39;, cbar_pad=0.15)
        else:
            res.append(plt.figure(figsize=(8, 7)))
            grid = ImageGrid(res[-1], 111, axes_pad=0.15, nrows_ncols=(1, 1), share_all=True,
                             cbar_location=&#39;right&#39;, cbar_mode=&#39;single&#39;, cbar_size=&#39;7%&#39;, cbar_pad=0.15)

        opts = {&#39;cmap&#39;: plt.get_cmap(
            &#39;coolwarm&#39;), &#39;vmin&#39;: -1, &#39;vmax&#39;: +1, &#39;snap&#39;: True}

        axs = grid[0]
        heatmap = axs.pcolor(mat, **opts)
        axs.set_title(lab, fontsize=14, fontweight=&#39;bold&#39;)

        lab = mat.columns.values

        # shift location of ticks to center of the bins
        axs.set_xticks(np.arange(len(lab)), minor=False)
        axs.set_yticks(np.arange(len(lab)), minor=False)
        axs.set_xticklabels(lab, minor=False, ha=&#39;left&#39;,
                            rotation=90, fontsize=10)
        axs.set_yticklabels(lab, minor=False, va=&#39;bottom&#39;, fontsize=10)
        axs.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, direction=&#34;in&#34;)

        for tick in axs.xaxis.get_minor_ticks():
            tick.tick1line.set_markersize(0)
            tick.tick2line.set_markersize(0)
            tick.label1.set_horizontalalignment(&#39;center&#39;)

        plt.colorbar(heatmap, axs.cax)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_distr"><code class="name flex">
<span>def <span class="ident">plot_distr</span></span>(<span>data_list, column=None, bins=50, labels=None, colors=None, **kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>Draw histograms comparing the distributions of each class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_list</code></strong> :&ensp;<code>TreeHandler, pandas.Dataframe</code> or <code>list</code> of <code>them</code></dt>
<dd>Contains a TreeHandler or a dataframe for each class</dd>
<dt><strong><code>column</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>them</code></dt>
<dd>Contains the name of the features you want to plot
Example: ['dEdx', 'pT', 'ct']. If None all the features
are selected</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code> or <code>sequence</code> of <code>scalars</code> or <code>str</code></dt>
<dd>If bins is an int, it defines the number of equal-width
bins in the given range (10, by default). If bins is a
sequence, it defines a monotonically increasing array of
bin edges, including the rightmost edge, allowing for
non-uniform bin widths.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>them</code></dt>
<dd>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>them</code></dt>
<dd>List of the colors to be used to fill the histograms</dd>
</dl>
<p>**kwds:
extra arguments are passed on to pandas.DataFrame.hist():
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html</a></p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code> of <code>matplotlib.axes.AxesSubplot</code></dt>
<dd>Distributions of the features for each class</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_distr(data_list, column=None, bins=50, labels=None, colors=None, **kwds):  # pylint: disable=too-many-branches
    &#34;&#34;&#34;
    Draw histograms comparing the distributions of each class.

    Parameters
    -----------------------------------------
    data_list: TreeHandler, pandas.Dataframe or list of them
        Contains a TreeHandler or a dataframe for each class

    column: str or list of them
        Contains the name of the features you want to plot
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]. If None all the features
        are selected

    bins: int or sequence of scalars or str
        If bins is an int, it defines the number of equal-width
        bins in the given range (10, by default). If bins is a
        sequence, it defines a monotonically increasing array of
        bin edges, including the rightmost edge, allowing for
        non-uniform bin widths.

    labels: str or list of them
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    colors: str or list of them
        List of the colors to be used to fill the histograms

    **kwds:
        extra arguments are passed on to pandas.DataFrame.hist():
        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html

    Returns
    -----------------------------------------
    out: numpy array of matplotlib.axes.AxesSubplot
        Distributions of the features for each class
    &#34;&#34;&#34;
    list_of_df = []
    if not isinstance(data_list, list):
        data_list = [data_list]
        labels = [labels]
        colors = [colors]
    if isinstance(data_list[0], hipe4ml.tree_handler.TreeHandler):
        for handl in data_list:
            list_of_df.append(handl.get_data_frame())
    else:
        list_of_df = data_list

    if column is not None:
        if not isinstance(column, (list, np.ndarray, pd.Index)):
            column = [column]

    else:
        column = list(list_of_df[0].columns)

    if labels is None:
        labels = [f&#39;class{i_class}&#39; for i_class, _ in enumerate(list_of_df)]

    if colors is None:
        colors = [None for i_class, _ in enumerate(list_of_df)]

    for i_class, (dfm, lab, col) in enumerate(zip(list_of_df, labels, colors)):
        if i_class == 0:
            axes = dfm.hist(column=column, bins=bins, label=lab, color=col, **kwds)
            axes = axes.flatten()
            axes = axes[:len(column)]
        else:
            dfm.hist(ax=axes, column=column, bins=bins, label=lab, color=col, **kwds)
    for axs in axes:
        axs.set_ylabel(&#39;Counts&#39;)
    axes[-1].legend(loc=&#39;best&#39;)
    if len(axes) == 1:
        axes = axes[0]
    return axes</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_feature_imp"><code class="name flex">
<span>def <span class="ident">plot_feature_imp</span></span>(<span>df_in, y_truth, model, labels=None, n_sample=10000, approximate=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the feature importance using the shap algorithm for
each feature. The calculation is performed on a subsample of the
input training/test set</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df_in</code></strong> :&ensp;<code>Pandas dataframe</code></dt>
<dd>Training or test set dataframe</dd>
<dt><strong><code>y_truth</code></strong> :&ensp;<code>array</code></dt>
<dd>Training or test set label</dd>
<dt><strong><code>model</code></strong> :&ensp;<code><a title="hipe4ml" href="index.html">hipe4ml</a> model_handler</code></dt>
<dd>Trained model</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</dd>
<dt><strong><code>n_sample</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of candidates employed to fill
the shap plots. If larger than the number of
candidates in each class, minimum number of candidates
in a given class used instead</dd>
<dt><strong><code>approximate</code></strong> :&ensp;<code>bool</code></dt>
<dd>Run fast and approximat roughly the SHAP values. For more information
see <a href="https://shap.readthedocs.io/en/latest/#shap.TreeExplainer.shap_values">https://shap.readthedocs.io/en/latest/#shap.TreeExplainer.shap_values</a></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>List</code> of <code>matplotlib.figure.Figure</code></dt>
<dd>Plots with shap feature importance. The first ones are the shap violin plots
computed for each class(in case of binary classification only one plot is returned).
The last plot of the list is built by taking the mean absolute value of the SHAP
values for each feature to get a standard bar plot (stacked bars are produced for
multi-class outputs)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_feature_imp(df_in, y_truth, model, labels=None, n_sample=10000, approximate=True):
    &#34;&#34;&#34;
    Calculate the feature importance using the shap algorithm for
    each feature. The calculation is performed on a subsample of the
    input training/test set

    Parameters
    -------------------------------------------
    df_in: Pandas dataframe
        Training or test set dataframe

    y_truth: array
        Training or test set label

    model: hipe4ml model_handler
        Trained model

    labels: list
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    n_sample: int
        Number of candidates employed to fill
        the shap plots. If larger than the number of
        candidates in each class, minimum number of candidates
        in a given class used instead

    approximate: bool
        Run fast and approximat roughly the SHAP values. For more information
        see https://shap.readthedocs.io/en/latest/#shap.TreeExplainer.shap_values

    Returns
    -------------------------------------------
    out: List of matplotlib.figure.Figure
        Plots with shap feature importance. The first ones are the shap violin plots
        computed for each class(in case of binary classification only one plot is returned).
        The last plot of the list is built by taking the mean absolute value of the SHAP
        values for each feature to get a standard bar plot (stacked bars are produced for
        multi-class outputs)

    &#34;&#34;&#34;
    class_labels, class_counts = np.unique(y_truth, return_counts=True)
    n_classes = len(class_labels)
    for class_count in class_counts:
        if n_sample &gt; class_count:
            n_sample = class_count

    subs = []
    for class_lab in class_labels:
        subs.append(df_in[y_truth == class_lab].sample(n_sample))

    df_subs = pd.concat(subs)
    df_subs = df_subs[model.get_training_columns()]
    explainer = shap.TreeExplainer(model.get_original_model())
    shap_values = explainer.shap_values(df_subs, approximate=approximate)
    res = []

    if n_classes &lt;= 2:
        res.append(plt.figure(figsize=(18, 9)))
        shap.summary_plot(shap_values, df_subs, plot_size=(
            18, 9), class_names=labels, show=False)
    else:
        for i_class in range(n_classes):
            res.append(plt.figure(figsize=(18, 9)))
            shap.summary_plot(shap_values[i_class], df_subs, plot_size=(
                18, 9), class_names=labels, show=False)

    res.append(plt.figure(figsize=(18, 9)))
    shap.summary_plot(shap_values, df_subs, plot_type=&#39;bar&#39;, plot_size=(
        18, 9), class_names=labels, show=False)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_learning_curves"><code class="name flex">
<span>def <span class="ident">plot_learning_curves</span></span>(<span>model, data, n_points=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot learning curves</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code><a title="hipe4ml" href="index.html">hipe4ml</a> model_handler</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>n_points</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of points used to sample the learning curves</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>Plot containing the learning curves</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_learning_curves(model, data, n_points=10):
    &#34;&#34;&#34; Plot learning curves

    Parameters
    -------------------------------------
    model: hipe4ml model_handler

    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    n_points: int
        Number of points used to sample the learning curves

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the learning curves
    &#34;&#34;&#34;

    res = plt.figure()
    train_errors, test_errors = [], []
    min_cand = 100
    max_cand = len(data[0])
    step = int((max_cand-min_cand)/n_points)
    array_n_cand = np.arange(start=min_cand, stop=max_cand, step=step)
    for n_cand in array_n_cand:
        model.fit(data[0][:n_cand], data[1][:n_cand])
        y_train_predict = model.predict(data[0][:n_cand], output_margin=False)
        y_test_predict = model.predict(data[2], output_margin=False)
        train_errors.append(mean_squared_error(
            y_train_predict, data[1][:n_cand], multioutput=&#39;uniform_average&#39;))
        test_errors.append(mean_squared_error(
            y_test_predict, data[3], multioutput=&#39;uniform_average&#39;))
    plt.plot(array_n_cand, np.sqrt(train_errors), &#39;r&#39;, lw=1, label=&#39;Train&#39;)
    plt.plot(array_n_cand, np.sqrt(test_errors), &#39;b&#39;, lw=1, label=&#39;Test&#39;)
    plt.ylim([0, np.amax(np.sqrt(test_errors))*2])
    plt.xlabel(&#39;Training set size&#39;)
    plt.ylabel(&#39;RMSE&#39;)
    plt.grid()
    plt.legend(loc=&#39;best&#39;)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_output_train_test"><code class="name flex">
<span>def <span class="ident">plot_output_train_test</span></span>(<span>model, data, bins=80, output_margin=True, labels=None, logscale=False, **kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the model output distributions for each class and output
both for training and test set.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code><a title="hipe4ml" href="index.html">hipe4ml</a> model handler</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code> or <code>sequence</code> of <code>scalars</code> or <code>str</code></dt>
<dd>If bins is an int, it defines the number of equal-width
bins in the given range (10, by default). If bins is a
sequence, it defines a monotonically increasing array of
bin edges, including the rightmost edge, allowing for
non-uniform bin widths.
If bins is a string, it defines the method used to
calculate the optimal bin width, as defined by
np.histogram_bin_edges:
<a href="https://docs.scipy.org/doc/numpy/reference/generated">https://docs.scipy.org/doc/numpy/reference/generated</a>
/numpy.histogram_bin_edges.html#numpy.histogram_bin_edges</dd>
<dt><strong><code>output_margin</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to output the raw untransformed margin value.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</dd>
<dt><strong><code>logscale</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to plot the y axis in log scale</dd>
<dt><strong><code>**kwds</code></strong></dt>
<dd>Extra arguments are passed on to plt.hist()</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code> or <code>list</code> of <code>them</code></dt>
<dd>Model output distributions for each class</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_output_train_test(model, data, bins=80, output_margin=True, labels=None, logscale=False, **kwds):
    &#34;&#34;&#34;
    Plot the model output distributions for each class and output
    both for training and test set.

    Parameters
    ----------------------------------------
    model: hipe4ml model handler

    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    bins: int or sequence of scalars or str
        If bins is an int, it defines the number of equal-width
        bins in the given range (10, by default). If bins is a
        sequence, it defines a monotonically increasing array of
        bin edges, including the rightmost edge, allowing for
        non-uniform bin widths.
        If bins is a string, it defines the method used to
        calculate the optimal bin width, as defined by
        np.histogram_bin_edges:
        https://docs.scipy.org/doc/numpy/reference/generated
        /numpy.histogram_bin_edges.html#numpy.histogram_bin_edges

    output_margin: bool
        Whether to output the raw untransformed margin value.

    labels: list
        Contains the labels to be displayed in the legend
        If None the labels are class1, class2, ..., classN

    logscale: bool
        Whether to plot the y axis in log scale

    **kwds
        Extra arguments are passed on to plt.hist()

    Returns
    ----------------------------------------
    out: matplotlib.figure.Figure or list of them
        Model output distributions for each class
    &#34;&#34;&#34;
    class_labels = np.unique(data[1])
    n_classes = len(class_labels)

    prediction = []
    for xxx, yyy in ((data[0], data[1]), (data[2], data[3])):
        for class_lab in class_labels:
            prediction.append(model.predict(
                xxx[yyy == class_lab], output_margin))

    low = min(np.min(d) for d in prediction)
    high = max(np.max(d) for d in prediction)
    low_high = (low, high)

    # only one figure in case of binary classification
    if n_classes &lt;= 2:
        res = plt.figure()
        labels = [&#39;Signal&#39;, &#39;Background&#39;] if labels is None else labels
        colors = [&#39;b&#39;, &#39;r&#39;]
        for i_class, (label, color) in enumerate(zip(labels, colors)):
            _plot_output(
                prediction[i_class], prediction[i_class+2], low_high, bins, label, color, kwds)
        if logscale:
            plt.yscale(&#39;log&#39;)
        plt.xlabel(&#39;BDT output&#39;, fontsize=13, ha=&#39;right&#39;, position=(1, 20))
        plt.ylabel(&#39;Counts (arb. units)&#39;, fontsize=13,
                   horizontalalignment=&#39;left&#39;)
        plt.legend(frameon=False, fontsize=12, loc=&#39;best&#39;)

    # n figures in case of multi-classification with n classes
    else:
        res = []
        labels = [
            f&#39;class{class_lab}&#39; for class_lab in class_labels] if labels is None else labels
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        colors = [cmap(i_class) for i_class in range(len(labels))]
        for output, out_label in zip(class_labels, labels):
            res.append(plt.figure())
            for i_class, (label, color) in enumerate(zip(labels, colors)):
                _plot_output(prediction[i_class][:, output], prediction[i_class+n_classes][:, output], low_high, bins,
                             label, color, kwds)
            if logscale:
                plt.yscale(&#39;log&#39;)
            plt.xlabel(f&#39;BDT output for {out_label}&#39;,
                       fontsize=13, ha=&#39;right&#39;, position=(1, 20))
            plt.ylabel(&#39;Counts (arb. units)&#39;, fontsize=13,
                       horizontalalignment=&#39;left&#39;)
            plt.legend(frameon=False, fontsize=12, loc=&#39;best&#39;)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_precision_recall"><code class="name flex">
<span>def <span class="ident">plot_precision_recall</span></span>(<span>y_truth, y_score, labels=None, pos_label=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot precision recall curve</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_truth</code></strong> :&ensp;<code>array</code></dt>
<dd>True labels for the belonging class. If labels are not
{0, 1, &hellip;, N}, then pos_label should be explicitly given.</dd>
<dt><strong><code>y_score</code></strong> :&ensp;<code>array</code></dt>
<dd>Estimated probabilities or decision function.</dd>
<dt><strong><code>pos_label</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>The label of the positive class. When pos_label=None,
if y_true is in {0, 1, &hellip;, N}, pos_label is set to 1,
otherwise an error will be raised.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>Plot containing the precision recall curves</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_precision_recall(y_truth, y_score, labels=None, pos_label=None):
    &#34;&#34;&#34; Plot precision recall curve

    Parameters
    -------------------------------------
    y_truth: array
        True labels for the belonging class. If labels are not
        {0, 1, ..., N}, then pos_label should be explicitly given.

    y_score: array
        Estimated probabilities or decision function.

    pos_label : int or str
        The label of the positive class. When pos_label=None,
        if y_true is in {0, 1, ..., N}, pos_label is set to 1,
        otherwise an error will be raised.

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the precision recall curves
    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(f&#39;class{i_class}&#39;)

    res = plt.figure()
    if n_classes &lt;= 2:
        precision, recall, _ = precision_recall_curve(
            y_truth, y_score, pos_label=pos_label)
        plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;)
        plt.fill_between(recall, precision, alpha=0.2, color=&#39;b&#39;, step=&#39;post&#39;)
        average_precision = average_precision_score(y_truth, y_score)
        plt.title(
            f&#39;2-class Precision-Recall curve: AP={average_precision:0.2f}&#39;)
    else:
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        precision, recall = (dict() for i_dict in range(2))
        # convert multi-class labels to multi-labels to obtain a curve for each class
        y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
        for clas, lab in enumerate(labels):
            precision[clas], recall[clas], _ = precision_recall_curve(
                y_truth_multi[:, clas], y_score[:, clas], pos_label=pos_label)
            plt.step(recall[clas], precision[clas], color=cmap(clas), lw=1, where=&#39;post&#39;,
                     label=lab)
        # compute also micro average
        precision[&#39;micro&#39;], recall[&#39;micro&#39;], _ = precision_recall_curve(
            y_truth_multi.ravel(), y_score.ravel())
        plt.step(recall[&#39;micro&#39;], precision[&#39;micro&#39;], color=&#39;black&#39;, where=&#39;post&#39;,
                 linestyle=&#39;--&#39;, lw=1, label=&#39;average&#39;)
        average_precision = average_precision_score(
            y_truth_multi, y_score, average=&#39;micro&#39;)
        plt.title(
            f&#39;Average precision score, micro-averaged over all classes: {average_precision:0.2f}&#39;)

    plt.xlabel(&#39;Recall&#39;)
    plt.ylabel(&#39;Precision&#39;)
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    if n_classes &gt; 2:
        plt.legend(loc=&#39;lower left&#39;)
        plt.grid()
    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_roc"><code class="name flex">
<span>def <span class="ident">plot_roc</span></span>(<span>y_truth, y_score, pos_label=None, labels=None, average='macro', multi_class_opt='raise')</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate and plot the roc curve</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_truth</code></strong> :&ensp;<code>array</code></dt>
<dd>True labels for the belonging class. If labels are not
{0, 1} in binary classification, then pos_label should
be explicitly given. In multi-classification labels must
be {0, 1, &hellip;, N}</dd>
<dt><strong><code>y_score</code></strong> :&ensp;<code>array</code></dt>
<dd>Target scores, can either be probability estimates or
non-thresholded measure of decisions (as returned
by “decision_function” on some classifiers).</dd>
<dt><strong><code>pos_label</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>The label of the positive class. Only available in binary
classification. When pos_label=None, if y_true is in {0, 1},
pos_label is set to 1, otherwise an error will be raised.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the labels to be displayed in the legend, used only in case of
multi-classification. They must be in the same order as the y_score columns.
If None the labels are class1, class2, &hellip;, classN</dd>
<dt><strong><code>average</code></strong> :&ensp;<code>string</code></dt>
<dd>Option for the average of ROC AUC scores used only in case of multi-classification.
You can choose between 'macro' and 'weighted'. For more information see
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score</a></dd>
<dt><strong><code>multi_class_opt</code></strong> :&ensp;<code>string</code></dt>
<dd>Option to compute ROC curves used only in case of multi-classification.
The one-vs-one 'ovo' and one-vs-rest 'ovr' approaches are available</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>Plot containing the roc curves</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_roc(y_truth, y_score, pos_label=None, labels=None, average=&#39;macro&#39;, multi_class_opt=&#39;raise&#39;):
    &#34;&#34;&#34;
    Calculate and plot the roc curve

    Parameters
    -------------------------------------
    y_truth: array
        True labels for the belonging class. If labels are not
        {0, 1} in binary classification, then pos_label should
        be explicitly given. In multi-classification labels must
        be {0, 1, ..., N}

    y_score: array
        Target scores, can either be probability estimates or
        non-thresholded measure of decisions (as returned
        by “decision_function” on some classifiers).

    pos_label: int or str
        The label of the positive class. Only available in binary
        classification. When pos_label=None, if y_true is in {0, 1},
        pos_label is set to 1, otherwise an error will be raised.

    labels: list
        Contains the labels to be displayed in the legend, used only in case of
        multi-classification. They must be in the same order as the y_score columns.
        If None the labels are class1, class2, ..., classN

    average: string
        Option for the average of ROC AUC scores used only in case of multi-classification.
        You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

    multi_class_opt: string
        Option to compute ROC curves used only in case of multi-classification.
        The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the roc curves
    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    res = plt.figure()
    if n_classes &lt;= 2:
        # binary case
        fpr, tpr, _ = roc_curve(y_truth, y_score, pos_label=pos_label)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, label=f&#39;ROC (AUC = {roc_auc:.4f})&#39;)

    else:
        # multi-class case
        if (labels is None) or (labels and len(labels) != n_classes):
            labels = [f&#39;class{i_class}&#39; for i_class in range(n_classes)]
        if multi_class_opt not in [&#39;ovo&#39;, &#39;ovr&#39;]:
            print(&#39;ERROR: if n_class &gt; 2 multi_class_opt must be ovo or ovr&#39;)
            return res

        # check to have numpy arrays
        if not isinstance(y_truth, np.ndarray):
            y_truth = np.array(y_truth)
        if not isinstance(y_score, np.ndarray):
            y_score = np.array(y_score)

        # if y_score contains raw outputs transform them to probabilities
        if not np.allclose(1, y_score.sum(axis=1)):
            y_score = softmax(y_score, axis=1)

        # one-vs-rest case
        if multi_class_opt == &#39;ovr&#39;:
            _plot_roc_ovr(y_truth, y_score, n_classes, labels, average)
        # one-vs-one case
        if multi_class_opt == &#39;ovo&#39;:
            _plot_roc_ovo(y_truth, y_score, n_classes, labels, average)

    plt.plot([0, 1], [0, 1], &#39;-.&#39;, color=(0.6, 0.6, 0.6), label=&#39;Luck&#39;)
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_roc_train_test"><code class="name flex">
<span>def <span class="ident">plot_roc_train_test</span></span>(<span>y_truth_test, y_score_test, y_truth_train, y_score_train, pos_label=None, labels=None, average='macro', multi_class_opt='raise')</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate and plot the roc curve for test and train sets</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>y_truth_test</code></strong> :&ensp;<code>array</code></dt>
<dd 0_="0," 1_="1," N class="..,">True labels for the belonging class of the test set. If labels
are not {0, 1} in binary classification, then pos_label should
be explicitly given. In multi-classification labels must be</dd>
<dt><strong><code>y_score_test</code></strong> :&ensp;<code>array</code></dt>
<dd>Target scores for the test set, can either be probability
estimates or non-thresholded measure of decisions (as returned
by “decision_function” on some classifiers).</dd>
<dt><strong><code>y_truth_train</code></strong> :&ensp;<code>array</code></dt>
<dd 0_="0," 1_="1," N class="..,">True labels for the belonging class of the train set. If labels
are not {0, 1} in binary classification, then pos_label should
be explicitly given. In multi-classification labels must be</dd>
<dt><strong><code>y_score_train</code></strong> :&ensp;<code>array</code></dt>
<dd>Target scores for the train set, can either be probability
estimates or non-thresholded measure of decisions (as returned
by “decision_function” on some classifiers).</dd>
<dt><strong><code>pos_label</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>The label of the positive class. Only available in binary
classification. When pos_label=None, if y_true is in {0, 1},
pos_label is set to 1, otherwise an error will be raised.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the labels to be displayed in the legend, used only in case of
multi-classification. They must be in the same order as the y_score columns.
If None the labels are class1, class2, &hellip;, classN</dd>
<dt><strong><code>average</code></strong> :&ensp;<code>string</code></dt>
<dd>Option for the average of ROC AUC scores used only in case of multi-classification.
You can choose between 'macro' and 'weighted'. For more information see
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score</a></dd>
<dt><strong><code>multi_class_opt</code></strong> :&ensp;<code>string</code></dt>
<dd>Option to compute ROC curves used only in case of multi-classification.
The one-vs-one 'ovo' and one-vs-rest 'ovr' approaches are available</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>Plot containing the roc curves</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_roc_train_test(y_truth_test, y_score_test, y_truth_train, y_score_train, pos_label=None, labels=None,
                        average=&#39;macro&#39;, multi_class_opt=&#39;raise&#39;):
    &#34;&#34;&#34;
    Calculate and plot the roc curve for test and train sets

    Parameters
    -------------------------------------
    y_truth_test: array
        True labels for the belonging class of the test set. If labels
        are not {0, 1} in binary classification, then pos_label should
        be explicitly given. In multi-classification labels must be
        {0, 1, ..., N}

    y_score_test: array
        Target scores for the test set, can either be probability
        estimates or non-thresholded measure of decisions (as returned
        by “decision_function” on some classifiers).

    y_truth_train: array
        True labels for the belonging class of the train set. If labels
        are not {0, 1} in binary classification, then pos_label should
        be explicitly given. In multi-classification labels must be
        {0, 1, ..., N}

    y_score_train: array
        Target scores for the train set, can either be probability
        estimates or non-thresholded measure of decisions (as returned
        by “decision_function” on some classifiers).

    pos_label: int or str
        The label of the positive class. Only available in binary
        classification. When pos_label=None, if y_true is in {0, 1},
        pos_label is set to 1, otherwise an error will be raised.

    labels: list
        Contains the labels to be displayed in the legend, used only in case of
        multi-classification. They must be in the same order as the y_score columns.
        If None the labels are class1, class2, ..., classN

    average: string
        Option for the average of ROC AUC scores used only in case of multi-classification.
        You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

    multi_class_opt: string
        Option to compute ROC curves used only in case of multi-classification.
        The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

    Returns
    -------------------------------------
    out: matplotlib.figure.Figure
        Plot containing the roc curves
    &#34;&#34;&#34;
    # call plot_roc for both train and test sets
    fig_test = plot_roc(y_truth_test, y_score_test,
                        pos_label, labels, average, multi_class_opt)
    fig_train = plot_roc(y_truth_train, y_score_train,
                         pos_label, labels, average, multi_class_opt)
    axes_test = fig_test.get_axes()[0]
    axes_train = fig_train.get_axes()[0]

    # plot results together
    res = plt.figure()
    for roc_test, roc_train in zip(axes_test.lines, axes_train.lines):
        test_label = roc_test.get_label()
        train_label = roc_train.get_label()
        if &#39;Luck&#39; in [test_label, train_label]:
            continue

        plt.plot(roc_test.get_xdata(), roc_test.get_ydata(), lw=roc_test.get_lw(), c=roc_test.get_c(),
                 alpha=roc_test.get_alpha(), marker=roc_test.get_marker(), linestyle=roc_test.get_linestyle(),
                 label=f&#39;Test -&gt; {test_label}&#39;)

        linestyle = roc_train.get_linestyle()
        if linestyle in &#39;-&#39;:
            linestyle = &#39;--&#39;
        plt.plot(roc_train.get_xdata(), roc_train.get_ydata(), lw=roc_train.get_lw(), c=roc_train.get_c(),
                 alpha=roc_train.get_alpha(), marker=roc_train.get_marker(), linestyle=linestyle,
                 label=f&#39;Train -&gt; {train_label}&#39;)

    plt.plot([0, 1], [0, 1], &#39;-.&#39;, color=(0.6, 0.6, 0.6), label=&#39;Luck&#39;)
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()

    return res</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hipe4ml" href="index.html">hipe4ml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hipe4ml.plot_utils.plot_bdt_eff" href="#hipe4ml.plot_utils.plot_bdt_eff">plot_bdt_eff</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_corr" href="#hipe4ml.plot_utils.plot_corr">plot_corr</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_distr" href="#hipe4ml.plot_utils.plot_distr">plot_distr</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_feature_imp" href="#hipe4ml.plot_utils.plot_feature_imp">plot_feature_imp</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_learning_curves" href="#hipe4ml.plot_utils.plot_learning_curves">plot_learning_curves</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_output_train_test" href="#hipe4ml.plot_utils.plot_output_train_test">plot_output_train_test</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_precision_recall" href="#hipe4ml.plot_utils.plot_precision_recall">plot_precision_recall</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_roc" href="#hipe4ml.plot_utils.plot_roc">plot_roc</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_roc_train_test" href="#hipe4ml.plot_utils.plot_roc_train_test">plot_roc_train_test</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>